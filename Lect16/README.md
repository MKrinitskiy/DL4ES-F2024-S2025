## ML4ES, Лекция 16

#### Функции активации: часть 2.



На лекции основное внимание уделялось функциям активации в нейронных сетях, обсуждались их типы, характеристики и оптимизация вычислений, в том числе при использовании графических процессоров (GPU). Были рассмотрены различные функции активации, такие как ReLU, GELU, сигмоида и гиперболический тангенс, с акцентом на их историческое развитие и популярность. Особо выделялась функция ReLU за ее простоту и эффективность, хотя она имеет проблемы, такие как «мертвые нейроны» и смещение среднего и дисперсии. Также были представлены более новые функции, такие как ELU (Exponential Linear Unit) и Swish, разработанные для решения некоторых ограничений традиционных функций. Новые функции, такие как Mish и Swish, оптимизированы для обеспечения более гладких ландшафтов потерь, что способствует более стабильному обучению. Лекция также затронула важность оптимизации вычислений нейронных сетей на GPU, подчеркнув, что такие вычисления в основном состоят из матричных умножений и функций, таких как экспоненты. Было отмечено, что важно следить за тем, чтобы данные и параметры сети находились в памяти GPU, чтобы избежать затратных передач данных, которые могут стать узким местом. Практические советы касались эффективного управления вычислительными ресурсами, особенно при работе с большими моделями и наборами данных, такими как современные лингвистические модели.



функции активации, нейронные сети, relu, оптимизация, gpu, elu, swish, mish, матричное умножение, распределение данных