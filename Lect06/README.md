## ML4ES, Лекция 6

#### Оптимизация нейросетей. Алгоритмы семейства стохастической градиентной оптимизации: SGD, Momentum, AdaGrad, Adam.



Лекция посвящена оптимизации нейронных сетей, с акцентом на минимизацию функции потерь методом градиентного спуска и его разновидностей, таких как стохастический градиентный спуск (SGD) и методы с импульсом. Основное внимание уделяется улучшению стабильности и скорости обучения за счет использования адаптивных методов, таких как AdaGrad и Adam, которые учитывают масштабы градиентов. Понятие эпохи объясняется как полный проход по всем данным, а батчи — как небольшие порции данных, используемые для обновления параметров. Импульсный метод помогает уменьшить осцилляции, а Adam объединяет импульсный и адаптивный подходы для более стабильного и быстрого обучения. Демонстрируется использование PyTorch для применения этих методов на практике.



нейронные сети, оптимизация, градиентный спуск, стохастический, эпоха, mini-batch, импульсный метод, adagrad, adam, pytorch