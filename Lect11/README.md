## ML4ES, Лекция 11

#### Роль начального приближения в задаче оптимизации глубоких искусственных нейронных сетей. Инициализации [Kaiming He](https://arxiv.org/abs/1502.01852), [Xavier Glorot](https://proceedings.mlr.press/v9/glorot10a.html).



В этой лекции основное внимание уделяется важности правильной инициализации параметров в нейронных сетях и влиянию этой инициализации на процесс обучения. Обсуждается, что если начальное приближение выбрано неправильно (например, все веса равны нулю или слишком малы), обучение может не начаться или пойдет очень медленно. Обсуждались различные методы инициализации, такие как инициализация [Xavier Glorot](https://proceedings.mlr.press/v9/glorot10a.html), которая помогает стабилизировать дисперсию активаций по мере продвижения вглубь сети, что важно для успешного обучения. Также упоминалась инициализация [Kaiming He](https://arxiv.org/abs/1502.01852) для функции активации ReLU, предложенная в 2015 году, которая учитывает особенности этой функции. Уделено внимание тому, что в современных фреймворках уже реализованы правильные методы инициализации для популярных функций активации, таких как ReLU. Было отмечено, что правильная инициализация является критически важной для успешного начала обучения нейронной сети и должна учитывать характеристики функций активации и архитектуру сети.



инициализация, нейросети, обучение, параметры, дисперсия, активация, градиент, xavier glorot, kaiming he, relu, pytorch