## ML4ES, Лекция 13

#### Шум в оптимизации нейросетей. Искусственное дополнение данных (Data augmentation).

Обсуждаются проблемы и методы оптимизации глубоких искусственных нейронных сетей, включая выбор начального приближения, где неправильный выбор может замедлить или остановить оптимизацию. Рассматривались методы первого порядка, такие как стохастический градиентный спуск, предпочтительные из-за меньшей вычислительной нагрузки по сравнению с методами второго порядка. Обсуждалась сложность поиска глобального минимума функции потерь из-за большого количества параметров и фрактального характера её ландшафта. Уделялось внимание стратегиям оптимизации, таким как использование Adam и его модификаций, нормирующих градиенты для стабилизации обучения. Нормализация активаций (batch normalization), помогает поддерживать стабильность сети. Поясняется влияние шума в оптимизации, который может как улучшать, так и ухудшать обучение в зависимости от ситуации. Обсуждается понятие обобщающей способности модели. Было упомянуто искусственное дополнение данных (data augmentation) как способ увеличения объема данных, что особенно полезно при их недостатке, путем добавления небольшого шума к исходным данным для сохранения их распределения. Лекция завершилась обсуждением практических аспектов обучения нейросетей и важности правильного управления шумом и структурой данных для успешной оптимизации.



оптимизация, нейронные сети, начальное приближение, градиентный спуск, нормализация, шум, обобщающая способность, data augmentation, Adam, функция потерь